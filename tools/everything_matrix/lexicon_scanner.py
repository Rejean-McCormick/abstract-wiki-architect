import os
import json
import glob

# Path to the central configuration
CONFIG_PATH = 'config/everything_matrix_config.json'

def load_config():
    if not os.path.exists(CONFIG_PATH):
        return {}
    with open(CONFIG_PATH, 'r') as f:
        return json.load(f)

def scan_lexicon():
    """
    Scans the existing data/lexicon folder structure.
    """
    config = load_config()
    
    # 1. Detect where the data actually lives
    # Legacy/Current path from your docs: data/lexicon/{lang_code}/...
    lex_root = "data/lexicon"
    imports_dir = "data/imports"
    mappings_dir = "semantics/mappings"
    
    scores = {}

    print(f"ðŸ” Scanning Lexicon Data in {lex_root}...")

    # --- BLOCK 1: LEXICON SEEDS (Existing Data) ---
    # We look for folders like data/lexicon/en/
    if os.path.exists(lex_root):
        # List all subdirectories in data/lexicon
        subdirs = [d for d in os.listdir(lex_root) if os.path.isdir(os.path.join(lex_root, d))]
        
        for lang_code in subdirs:
            # We assume folder name is the code (e.g. 'en', 'fr', 'Eng')
            # Normalize to Title Case if it's 3 letters (eng -> Eng), else keep as is
            # (You might need a map here later, but this is a good heuristic)
            wiki_code = lang_code.capitalize() if len(lang_code) == 3 else lang_code
            
            lang_path = os.path.join(lex_root, lang_code)
            
            # Check if it has content (core.json, people.json, etc.)
            json_files = glob.glob(os.path.join(lang_path, "*.json"))
            
            if json_files:
                if wiki_code not in scores: scores[wiki_code] = {}
                
                # Score based on file count? Or just existence?
                # Let's give 10 if 'core.json' exists, else 5
                has_core = any("core.json" in f for f in json_files)
                scores[wiki_code]["lex_seed"] = 10 if has_core else 5

    # --- BLOCK 2: WIDE IMPORTS ---
    if os.path.exists(imports_dir):
        files = glob.glob(os.path.join(imports_dir, "*_wide.csv"))
        for f in files:
            code = os.path.basename(f).replace("_wide.csv", "")
            # Normalize 'eng' -> 'Eng' if needed
            if len(code) == 3: code = code.capitalize()
            
            if code not in scores: scores[code] = {}
            scores[code]["lex_wide"] = 10

    # --- BLOCK 3: SEMANTIC MAPPINGS ---
    if os.path.exists(mappings_dir):
        files = glob.glob(os.path.join(mappings_dir, "*.json"))
        for f in files:
            code = os.path.splitext(os.path.basename(f))[0]
            if len(code) == 3: code = code.capitalize()
            
            if code not in scores: scores[code] = {}
            scores[code]["sem_mappings"] = 10

    # --- BLOCK 4: CONCRETE DICTIONARIES (Compiled) ---
    # Check for Wiki{Lang}.gf files generated by the builder
    for f in glob.glob("Wiki*.gf"):
        if f == "Wiki.gf": continue
        code = f.replace("Wiki", "").replace(".gf", "")
        
        if code not in scores: scores[code] = {}
        scores[code]["lex_concrete"] = 10

    return scores

if __name__ == "__main__":
    results = scan_lexicon()
    print(json.dumps(results, indent=2))
    print(f"âœ… Scanned lexicon for {len(results)} languages.")