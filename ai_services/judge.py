import json
import logging
import re
from typing import Dict, Any, Optional
import httpx
import google.generativeai as genai

from app.shared.config import settings
from ai_services.prompts import JUDGE_SYSTEM_PROMPT

# Setup Logger
logger = logging.getLogger(settings.OTEL_SERVICE_NAME)

class JudgeAgent:
    """
    The Judge: A strict QA evaluator for Natural Language Generation.
    1. Evaluates outputs against Gold Standard data.
    2. Auto-files GitHub issues for regressions (The Whistleblower).
    """

    def __init__(self):
        self.api_key = settings.GOOGLE_API_KEY
        self.model_name = settings.AI_MODEL_NAME
        self.github_token = settings.GITHUB_TOKEN
        self.repo_url = settings.REPO_URL
        self._client = None

        if self.api_key:
            genai.configure(api_key=self.api_key)
            self._client = genai.GenerativeModel(self.model_name)
        else:
            logger.warning("âš ï¸ GOOGLE_API_KEY not found. The Judge is presiding in silent mode.")

    def evaluate_case(self, generated_text: str, gold_case: Dict[str, Any]) -> Dict[str, Any]:
        """
        Compares the engine's output against the Gold Standard 'expected' string.
        
        Args:
            generated_text: The actual output from the Abstract Wiki Engine.
            gold_case: A dictionary from gold_standard.json containing 'intent' and 'expected'.
            
        Returns:
            Dict: { "score": 0.0-1.0, "verdict": "PASS/FAIL", "critique": "..." }
        """
        if not self._client:
            return {"score": 0, "verdict": "SKIPPED", "critique": "AI Agent disabled"}

        expected_text = gold_case.get("expected")
        lang = gold_case.get("lang")
        intent = json.dumps(gold_case.get("intent"))

        # Zero-shot check: Exact string match (Fast Pass)
        if generated_text.strip().lower() == expected_text.strip().lower():
            return {
                "score": 1.0,
                "verdict": "PASS",
                "critique": "Exact match with Gold Standard."
            }

        # AI Evaluation (Slow Pass)
        user_prompt = f"""
        **CONTEXT:**
        Target Language: {lang}
        Semantic Intent: {intent}
        
        **COMPARISON:**
        Gold Standard: "{expected_text}"
        Actual Output: "{generated_text}"
        """

        try:
            response = self._client.generate_content(
                contents=[
                    {"role": "user", "parts": [JUDGE_SYSTEM_PROMPT + "\n\n" + user_prompt]}
                ],
                generation_config={"temperature": 0.0} # Deterministic grading
            )
            
            result = self._parse_json(response.text)
            
            # Auto-Report Regressions if configured
            if result.get("score", 0) < 0.8 and self.github_token:
                self._file_github_issue(gold_case, generated_text, result)

            return result

        except Exception as e:
            logger.error(f"âŒ Judge evaluation failed: {e}")
            return {"score": 0, "verdict": "ERROR", "critique": str(e)}

    def _file_github_issue(self, gold_case: Dict, actual: str, evaluation: Dict) -> None:
        """
        The Whistleblower: Files a GitHub issue when quality drops below threshold.
        """
        if "github.com" not in self.repo_url:
            return

        # Extract owner/repo from URL (e.g., https://github.com/owner/repo)
        try:
            _, _, _, owner, repo = self.repo_url.rstrip("/").split("/")[:5]
            api_url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        except ValueError:
            logger.error(f"Invalid REPO_URL format: {self.repo_url}")
            return

        title = f"Regression: {gold_case.get('lang')} - {gold_case.get('id')}"
        body = f"""
### ðŸš¨ Quality Regression Detected
**The Judge** has detected a failure in the Gold Standard test suite.

| Field | Value |
| :--- | :--- |
| **Test ID** | `{gold_case.get('id')}` |
| **Language** | `{gold_case.get('lang')}` |
| **Expected** | `{gold_case.get('expected')}` |
| **Actual** | `{actual}` |
| **Score** | `{evaluation.get('score')}` |

**Critique:**
> {evaluation.get('critique')}

*Auto-generated by Abstract Wiki Architect v2.0*
        """

        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }

        try:
            # Check for existing open issues with same title to avoid duplicates
            check_res = httpx.get(api_url, headers=headers, params={"state": "open", "labels": "regression"})
            if check_res.status_code == 200:
                for issue in check_res.json():
                    if issue["title"] == title:
                        logger.info(f"Existing issue found for {title}. Skipping.")
                        return

            # Create Issue
            res = httpx.post(api_url, headers=headers, json={
                "title": title,
                "body": body,
                "labels": ["regression", "automated", "judge"]
            })
            if res.status_code == 201:
                logger.info(f"ðŸ“‚ GitHub Issue filed: {title}")
            else:
                logger.error(f"Failed to file issue: {res.text}")

        except Exception as e:
            logger.error(f"Whistleblower error: {e}")

    def _parse_json(self, text: str) -> Dict[str, Any]:
        """
        Robust JSON extraction from LLM output.
        """
        try:
            # remove markdown fences
            clean = re.sub(r"```json|```", "", text).strip()
            return json.loads(clean)
        except json.JSONDecodeError:
            logger.error(f"Judge output invalid JSON: {text}")
            return {"score": 0, "verdict": "JSON_ERROR", "critique": "Output parsing failed"}

# Global Singleton
judge = JudgeAgent()